{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tanyatandon/Desktop/TextAnalyticsHW/20_newsgroups/alt.atheism\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cd ./20_newsgroups/alt.atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanyatandon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tanyatandon/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import glob\n",
    "import codecs\n",
    "\n",
    "import nltk\n",
    "import spacy \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from spacy.lang.en import English\n",
    "import stanfordnlp\n",
    "\n",
    "import re\n",
    "\n",
    "import multiprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 files\n",
      "Corpus is 2558273 characters long\n"
     ]
    }
   ],
   "source": [
    "# import corpus\n",
    "filenames = sorted(glob.glob(\"*\"))\n",
    "print(\"Found {} files\".format(len(filenames)))\n",
    "\n",
    "corpus_raw = u\"\"\n",
    "for filename in filenames:\n",
    "    with codecs.open(filename, 'rb', 'utf-8',errors='ignore') as news_file:\n",
    "        corpus_raw += news_file.read()\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentence using nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.571128\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.datetime.now() \n",
    "tokens_sentence = sent_tokenize(corpus_raw)\n",
    "time_end = datetime.datetime.now()\n",
    "\n",
    "time_diff = time_end - time_start\n",
    "\n",
    "print(time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize words using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:04.244708\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.datetime.now() \n",
    "tokens_word = word_tokenize(corpus_raw)\n",
    "time_end = datetime.datetime.now()\n",
    "\n",
    "time_diff = time_end - time_start\n",
    "\n",
    "print(time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T02:46:12.999569Z",
     "start_time": "2019-10-12T02:46:05.274601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:07.716645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_start = datetime.datetime.now()     \n",
    "ps = PorterStemmer() \n",
    "for w in tokens_word:\n",
    "    ps.stem(w)\n",
    "    #print(w, \" : \", ps.stem(w)) \n",
    "time_end = datetime.2datetime.now()    \n",
    "\n",
    "time_diff = time_end - time_start\n",
    "\n",
    "print(time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pos tagging using nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:24.671554\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_start = datetime.datetime.now()     \n",
    "stem_words = nltk.pos_tag(tokens_word)\n",
    "time_end = datetime.datetime.now()    \n",
    "\n",
    "time_diff = time_end - time_start\n",
    "\n",
    "print(time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentence using spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.874587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_raw_short = corpus_raw[0:1000000]\n",
    "time_start = datetime.datetime.now()  \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "doc = nlp(corpus_raw_short)\n",
    "corpus_tokenized_sen =  [sent.string.strip() for sent in doc.sents]\n",
    "\n",
    "time_end = datetime.datetime.now()    \n",
    "time_diff = time_end - time_start\n",
    "print(time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:05:09.215583Z",
     "start_time": "2019-10-11T17:05:09.188156Z"
    }
   },
   "source": [
    "Word tokenize using spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T02:51:26.822913Z",
     "start_time": "2019-10-12T02:50:39.128383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:47.671939\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.datetime.now()  \n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(corpus_raw_short)\n",
    "corpus_tokenized_word =  [sent.string.strip() for sent in doc]\n",
    "time_end = datetime.datetime.now()    \n",
    "time_diff = time_end - time_start\n",
    "print(time_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:06:56.522864Z",
     "start_time": "2019-10-11T17:06:56.516786Z"
    }
   },
   "source": [
    "Stemming using spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T02:51:37.130212Z",
     "start_time": "2019-10-12T02:51:33.971780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.150356\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_start = datetime.datetime.now()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "for token in corpus_tokenized_word:\n",
    "    stemmer.stem(token)\n",
    "    #print(token + ' --> ' + stemmer.stem(token))\n",
    "    \n",
    "time_end = datetime.datetime.now()    \n",
    "time_diff = time_end - time_start\n",
    "print(time_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T17:13:00.549226Z",
     "start_time": "2019-10-11T17:12:57.114Z"
    }
   },
   "source": [
    "Pos tagging using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T02:52:25.396139Z",
     "start_time": "2019-10-12T02:52:25.289585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.101417\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.datetime.now()\n",
    "corpus_pos_tagged = []\n",
    "for token in doc:\n",
    "    token.pos_\n",
    "    #print(str(token) + ' --> ' + token.pos_)\n",
    "time_end = datetime.datetime.now()    \n",
    "time_diff = time_end - time_start\n",
    "print(time_diff)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and test regular expressions for the following tasks:\n",
    " \n",
    "\n",
    "2.1 Match all emails in text and compile a set of all found email addresses.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T04:11:36.637164Z",
     "start_time": "2019-10-12T04:11:36.488990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mathew@mantis.co.uk',\n",
       " '19930329115719@mantis.co.uk',\n",
       " 'news-answers-request@mit.edu',\n",
       " '19930301143317@mantis.co.uk',\n",
       " 'figmo@netcom.com']"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = re.findall( r'([a-zA-Z0-9\\._.+-]+@[\\w\\.]+[\\w\\.])', corpus_raw)\n",
    "email[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.2 Find all dates in text (e.g. 04/12/2019, April 20th 2019, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T04:09:26.190370Z",
     "start_time": "2019-10-12T04:09:25.045619Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_dates = []\n",
    "\n",
    "date5 = re.findall(\"([\\d]{1,2}\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\\s[\\d]{4})\", corpus_raw)\n",
    "list_of_dates.append(date5)\n",
    "\n",
    "\n",
    "date6 = re.findall(\"((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\\s[\\d]{1,2}(th)*\\s[\\d]{4})\", corpus_raw)\n",
    "list_of_dates.append(date6)\n",
    "\n",
    "date7 = re.findall(\"((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\\s[\\d]{1,2}(th)*\\s[\\d]{2})\", corpus_raw)\n",
    "list_of_dates.append(date7)\n",
    "\n",
    "date1 = re.findall(r'\\d{0,1}\\d{1}[-\\s]\\d{0,1}\\d{1}[-\\s]\\d{2}', corpus_raw)\n",
    "list_of_dates.append(date1)\n",
    "date2 = re.findall(r'\\d{0,1}\\d{1}[/.\\s]\\d{0,1}\\d{1}[/.\\s]\\d{2}', corpus_raw)\n",
    "list_of_dates.append(date2)\n",
    "date3 = re.findall(r'\\d{0,1}\\d{1}[-\\s]\\d{0,1}\\d{1}[-\\s]\\d{4}', corpus_raw)\n",
    "list_of_dates.append(date3)\n",
    "date4 = re.findall(r'\\d{0,1}\\d{1}[/.\\s]\\d{0,1}\\d{1}[/.\\s]\\d{4}', corpus_raw)\n",
    "list_of_dates.append(date4)\n",
    "\n",
    "from itertools import chain\n",
    "list_of_dates = list(chain.from_iterable(list_of_dates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T04:09:37.016639Z",
     "start_time": "2019-10-12T04:09:37.009120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29 Mar 1993', '29 Apr 1993', '11 December 1992', '5 Apr 1993', '6 May 1993']"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dates [ 0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
